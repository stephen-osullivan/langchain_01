# Huggingface local model + Langchain + FastAPI + Streamlit

We download gemma-2b and llama2-7b from huggingface and then create an endpoint with fastapi and langserve and then access via streamlit.

We allow the user to specify if what model to run and whether to quantize when running the app.